1.并发整体架构图：参考J.U.C_2.png。

2.原子操作：Atomic。

(1)AtomicInteger、AtomicLong、AtomicBoolean、AtomicReference。

(2)AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray。

(3)AtomicIntegerFieldUpdater<T>、AtomicLongFieldUpdater<T>、AtomicReferenceFieldUpdater<T,V>。

(4)AtomicMarkableReference、AtomicStampedReference。

3.定义：

(1)线程安全：当多个线程访问一个类时，如果不用考虑这些线程在运行时环境下的调度和交替运行，并且不需要额外的同步及在调用方代码不必做其他的协调，这个类的行为仍然是正确的，那么这个类就是线程安全的。

(2)只有资源在竞争时才会导致线程不安全，因此无状态对象永远是线程安全的。

(3)原子操作：多个线程执行一个操作时，其中任何一个线程要么完全执行此操作，要么没有执行此操作的任何步骤，那么这个操作就是原子的。

(4)指令重排序：只要程序的最终结果等同于它在严格的顺序化环境下的结果，那么指令的执行顺序就可能与代码的顺序不一致，这个过程叫做指令的重排序。

(5)Happens-before法则：如果前一个操作Happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见。

(6)volatile：

内存模型相关概念：每一个线程都拥有自己的工作内存(相当于主存的一个副本)，操作完工作内存后再更新到主存中。

并发编程中的原子性、可见性、有序性。要想并发程序正确地执行，必须要保证原子性、可见性、有序性。只要有一个没有保证，就有可能导致程序运行不正确。

volatile的作用：

第一：使用volatile关键字，会强制将修改的值写入主存。
第二：使用volatile关键字，当前线程修改值的时候，会使其他线程中的值无效。
第三：由于其他线程中的值变得无效了，那么其他线程会重新从主存中读取值。

volatile可以保证可见性和一定程度的有序性，但是不能保证原子性。

4.

volatile不能保证原子性，因此对于同步最终还是要回到锁机制上来。

锁机制存在的问题：

(1)在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，引起性能问题。
(2)一个线程持有锁会导致其它所有需要此锁的线程挂起。
(3)如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置，引起性能风险。

乐观锁和悲观锁：

悲观锁：

总是悲观地假设最坏的情况，认为每次去拿数据的时候都有其他线程在修改数据，所以每次在拿数据的时候都会上锁，这样其他线程想要取这个数据的时候就会阻塞直到获得锁。

应用场景：(1)传统关系型数据库：行锁、表锁、读锁、写锁。(2)synchronized关键字。

乐观锁：一种思想。

总是乐观地认为每次去拿数据的时候没有其他线程在修改数据，不会上锁，但是每次在更新数据的时候会判断一下在此期间有没有其他线程更新此数据，如果有并发冲突就重试，直到成功为止。

适用于多读的场景。

CAS(Compare and Swap)：是乐观锁的一种实现方式。CAS操作中包含三个操作数，需要读写的内存位置(V)，进行比较的预期原值(A)和拟写入的新值(B)。如果内存位置V的值与预期原值A相匹配，那么处理器会自动将该位置值更新为新值B，
否则处理器不做任何操作。即：我认为位置V应该包含值A，如果包含该值，则将B放到这个位置。否则，不要更改该位置，只告诉我这个位置现在的值即可。

相对于synchronized这种阻塞算法，CAS是非阻塞算法的一种常见实现。CAS可能造成ABA问题，jdk1.5后，java.util.concurrent采用这种方式性能有了提升。jdk1.6后，资源竞争少的情况下，synchronized的性能接近CAS，资源竞争激烈的情况下，synchronized的性能远高于CAS。

ABA问题：

比如说一个线程one从内存位置V中取出A，这时候另一个线程two也从内存中取出A，并且two进行了一些操作变成了B，然后two又将V位置的数据变成A，这时候线程one进行CAS操作发现内存中仍然是A，然后one操作成功。尽管线程one的CAS操作成功，但是不代表这个过程就是没有问题的。如果链表的头在变化了两次后恢复了原值，但是不代表链表就没有变化。使用原子操作AtomicStampedReference/AtomicMarkableReference，允许一对变化的元素进行原子操作。

5.锁机制：

(1)
JNI：Java Native Interface，Java本地接口调用。

显式锁：jdk1.5后提供了接口，java.util.concurrent.locks.Lock 和 java.util.concurrent.locks.ReadWriteLock。

常见的显式锁写法：

Lock lock = new ReentrantLock();
lock.lock();
try{
    XXX;
}finally {
    lock.unlock();
}

synchronized使用的内置锁和ReentrantLock这种显式锁在jdk1.6之后的性能非常接近，甚至在更新的版本中内置锁的性能更好。使用显式锁的唯一理由是能够提供更多的功能。

(2)AQS：

java.util.concurrent.locks.AbstractQueuedSynchronizer基础类。

基本的思想表现为一个同步器：支持获取锁和释放锁。

获取锁：首先判断当前状态是否允许获取锁，如果是就获取锁，否则就阻塞操作(独占锁)或者获取失败(共享锁)。如果是阻塞线程，那么线程就需要进入阻塞队列。当状态位允许获取锁时就修改状态，并且如果进了队列就从队列中移除。

while(synchronization state does not allow acquire){

    enqueue current thread if not already queued;

    possibly block current thread;

}

dequeue current thread if it was queued;

释放锁：这个过程就是修改状态位，如果有线程因为状态位阻塞的话就唤醒队列中的一个或者更多线程。

update synchronization state;

if(state may permit a blocked thread to acquire)

    unlock one or more queued threads;
	
实现获取锁和释放锁必须有如下条件：

①原子性操作同步器的状态位：

32位版本的同步器使用32位的整数来描述状态位，使用CAS操作来解决这个问题。事实上还有64位版本的同步器，暂略。

②阻塞和唤醒队列：

jdk5.0以后利用JNI在LockSupport类中实现了此特性。

LockSupport.park()
LockSupport.park(Object)
LockSupport.parkNanos(Object, long)
LockSupport.parkNanos(long)
LockSupport.parkUntil(Object, long)
LockSupport.parkUntil(long)
LockSupport.unpark(Thread)

park()是在当前线程中调用，导致线程阻塞，带参数的Object是挂起的对象，这样监视的时候就能够知道此线程是因为什么资源而阻塞的。由于park()立即返回，所以通常情况下需要在循环中去检测竞争资源来决定是否进行下一次阻塞。
park()返回的原因有：

其他某个线程调用将当前线程作为目标调用 unpark。
其他某个线程中断当前线程。
该调用不合逻辑地(即毫无理由地)返回。

第三条就决定了需要循环检测了，类似于通常写的while(checkCondition()){Thread.sleep(time);}类似的功能。

③有序队列：

在AQS中采用CHL列表来解决有序的队列的问题。

CHL模型采用下面的算法完成FIFO的入队列和出队列过程。

对于入队列：

采用CAS操作，每次比较尾结点是否一致，然后插入到尾结点中。

对于出队列：

由于每一个节点也缓存了一个状态，决定是否出队列，因此当不满足条件时就需要自旋等待，一旦满足条件就将头结点设置为下一个节点。这里的自旋等待也是用 LockSupport.park() 来实现的。

AQS里面有三个核心字段：

private volatile int state;

private transient volatile Node head;

private transient volatile Node tail;

state描述的有多少个线程获得了锁，对于互斥锁来说state<=1。head/tail加上CAS操作就构成了一个CHL的FIFO队列。

下面是Node节点的属性：

volatile int waitStatus; 节点的等待状态，一个节点可能位于以下几种状态：

--CANCELLED = 1：节点操作因为超时或者对应的线程被interrupt。节点不应该留在此状态，一旦达到此状态将从CHL队列中踢出。
--SIGNAL = -1：节点的继任节点是(或者将要成为)BLOCKED状态(例如通过LockSupport.park()操作)，因此一个节点一旦被释放(解锁)或者取消就需要唤醒(LockSupport.unpack())它的继任节点。
--CONDITION = -2：表明节点对应的线程因为不满足一个条件(Condition)而被阻塞。
--0： 正常状态，新生的非CONDITION节点都是此状态。
--非负值标识节点不需要被通知(唤醒)。

volatile Node prev; 此节点的前一个节点。节点的waitStatus依赖于前一个节点的状态。

volatile Node next; 此节点的后一个节点。后一个节点是否被唤醒(uppark())依赖于当前节点是否被释放。

volatile Thread thread; 节点绑定的线程。

Node nextWaiter;下一个等待条件(Condition)的节点，由于Condition是独占模式，因此这里有一个简单的队列来描述Condition上的线程节点。

(3)加锁原理：

公平锁和非公平锁：

如果获取一个锁是按照请求的顺序得到的，那么就是公平锁，否则就是非公平锁。

公平锁保证一个阻塞的线程最终能够获得锁，因为是有序的，所以总是可以按照请求的顺序获得锁。不公平锁意味着后请求锁的线程可能在其前面排列的休眠线程恢复前拿到锁。这样就有可能提高并发性能，通常情况下挂起的线程重新开始与它真正开始运行，二者之间会产生严重的延时，非公平锁就可以利用这段时间完成操作。所以很多时候非公平锁的性能要比公平锁好。

锁机制原理：

这部分难度太大，暂略。

(4)锁工具：

①闭锁：一种同步方法，可以延迟进程的进度直到线程到达某个终点状态。通俗理解：类似于一扇门，门打开前，所有线程在门前等待，一旦门打开，所有线程都通过了，门的状态也不会变了，一直是打开状态。闭锁的状态是一次性的。

CountDownLatch：

public void await() throws InterruptedException
public boolean await(long timeout, TimeUnit unit) throws InterruptedException
public void countDown()
public long getCount()

独占锁(排它锁、互斥锁)：独占锁锁定的资源只允许进行锁定操作的线程使用。
共享锁：所有共享锁的线程共享同一个资源，一旦任意一个线程拿到共享资源，那么所有线程就都拥有同一份资源。通常情况下，共享锁只是一个标识，所有线程都在等待这个标识是否满足，一旦满足，所有线程都会被激活。闭锁就是共享锁的一种实现。

final CountDownLatch startLatch = new CountDownLatch(1);
final CountDownLatch overLatch = new CountDownLatch(times);
...
startLatch.await();

②CyclicBarrier：CountDownLatch是一次性的。CyclicBarrier可以循环使用，允许一组线程互相等待，直到到达某个公共屏障点，所谓屏障点就是一组任务执行完毕的时刻。

CyclicBarrier有以下特点：

await()方法将挂起线程，直到同组的其它线程执行完毕才能继续。
await()方法返回线程执行完毕的索引，注意，索引时从任务数-1开始的，也就是第一个执行完成的任务索引为parties-1,最后一个为0，这个parties为总任务数，清单中是cnt+1。
CyclicBarrier 是可循环的，显然名称说明了这点。在清单1中，每一组任务执行完毕就能够执行下一组任务。
如果屏障操作不依赖于挂起的线程，那么任何线程都可以执行屏障操作。在清单1中可以看到并没有指定那个线程执行50%、30%、0%的操作，而是一组线程(cnt+1)个中任何一个线程只要到达了屏障点都可以执行相应的操作。
CyclicBarrier 的构造函数允许携带一个任务，这个任务将在0%屏障点执行，它将在await()==0后执行。
CyclicBarrier 如果在await时因为中断、失败、超时等原因提前离开了屏障点，那么任务组中的其他任务将立即被中断，以InterruptedException异常离开线程。
所有await()之前的操作都将在屏障点之前运行，也就是CyclicBarrier 的内存一致性效果.

CyclicBarrier的API：

public CyclicBarrier(int parties)：创建一个新的 CyclicBarrier，它将在给定数量的参与者(线程)处于等待状态时启动，但它不会在启动 barrier 时执行预定义的操作。
public CyclicBarrier(int parties, Runnable barrierAction)：创建一个新的 CyclicBarrier，它将在给定数量的参与者(线程)处于等待状态时启动，并在启动 barrier 时执行给定的屏障操作，该操作由最后一个进入 barrier 的线程执行。
public int await() throws InterruptedException, BrokenBarrierException：在所有参与者都已经在此 barrier 上调用 await 方法之前，将一直等待。
public int await(long timeout,TimeUnit unit) throws InterruptedException, BrokenBarrierException,TimeoutException：在所有参与者都已经在此屏障上调用 await 方法之前将一直等待,或者超出了指定的等待时间。
public int getNumberWaiting()：返回当前在屏障处等待的参与者数目。此方法主要用于调试和断言。
public int getParties()：返回要求启动此 barrier 的参与者数目。
public boolean isBroken()：查询此屏障是否处于损坏状态。
public void reset()：将屏障重置为其初始状态。

CyclicBarrier的 reset 方法，手动将所有线程中断，恢复屏障点，进行下一组任务的执行。

③Semaphore：信号量。

Semaphore通俗地讲就是一个计数器，在计数器不为0的时候对线程放行，一旦为0，那么所有请求资源的新线程都会被阻塞，包括增加请求到许可的线程。信号量是不可重入的，每一次请求一个许可都会导致计数器减少1，同样每次释放一个许可都会导致计数器增加1，信号量为0时，新的许可请求线程将被挂起。

缓存池正好就是用这种思想实现的，如：链接池、对象池等。

信号量只是在信号不够的时候挂起线程，但是并不能保证信号量足够的时候获取对象和返回对象是线程安全的。

二进制信号量：将信号量初始化为1，使得在使用时最多只有一个可用的许可，从而可用作一个互相排斥的"锁"，二进制信号量可以由线程释放"锁"，而不是由所有者来释放。

和公平锁/非公平锁一样，信号量也有公平性。如果一个信号量是公平的，那么表示线程在获取信号量时按照FIFO的顺序得到许可。有可能某个线程先请求信号量而后进入请求队列，那么此线程获取信号量的顺序就会晚于其后请求但先进入请求队列的线程。此时信号量就是非公平性的。

④读写锁：

ReentrantLock实现了标准的互斥操作，即所谓的独占锁。独占锁是一种很保守的策略，在这种情况下"读/读"、"写/读"、"写/写"操作都不能同时发生。并发比较大的时候锁的开销也比较大，所以条件允许的情况下尽量少使用锁，一定要使用锁的时候看看能不能替换为读写锁。

ReadWriteLock描述的是：一个资源能够被多个读线程访问，或者被一个写线程访问，但是不能同时存在读写线程。也就是说读写锁的使用场景是一个共享资源被大量读取、少量修改的场景。

ReadWriteLock并不是Lock的子接口，只不过ReadWriteLock借助Lock来实现读写两个视角。在ReadWriteLock中每次读取共享数据就需要读取锁，当需要修改共享数据时就需要写入锁。看起来好像是两个锁，但其实不是这样，读写锁是独占锁的两个视图。

如：

final ReadWriteLock lock = new ReentrantReadWriteLock();
final Lock r = lock.readLock();
final Lock w = lock.writeLock();

r.lock();
try {
    //读操作
} finally {
    r.unlock();
}

w.lock();
try {
    //写操作
} finally {
    w.unlock();
}

ReadWriteLock需要严格区分读写操作，不能在读操作中使用写入锁，否则会发生错误。

ReentrantReadWriteLock 特性：

公平性：
	非公平锁(默认)：和独占锁的非公平性一样，由于读线程之间没有锁竞争，所以读操作没有公平性和非公平性，写操作时，由于写操作可能立即获取到锁，所以会推迟一个或多个读操作或者写操作。因此非公平锁的吞吐量要高于公平锁。	公平锁：利用AQS的CLH队列，释放当前保持的锁（读锁或者写锁）时，优先为等待时间最长的那个写线程分配写入锁，当前前提是写线程的等待时间要比所有读线程的等待时间要长。同样一个线程持有写入锁或者有一个写线程已经在等待了，那么试图获取公平锁的（非重入）所有线程（包括读写线程）都将被阻塞，直到最先的写线程释放锁。如果读线程的等待时间比写线程的等待时间还有长，那么一旦上一个写线程释放锁，这一组读线程将获取锁。

重入性：
	读写锁允许读线程和写线程按照请求锁的顺序重新获取读取锁或者写入锁。当然了只有写线程释放了锁，读线程才能获取重入锁。
	写线程获取写入锁后可以再次获取读取锁，但是读线程获取读取锁后却不能获取写入锁。
	另外读写锁最多支持65535个递归写入锁和65535个递归读取锁。

锁降级：
	写线程获取写入锁后可以获取读取锁，然后释放写入锁，这样就从写入锁变成了读取锁，从而实现锁降级的特性。

锁升级：
	读取锁是不能直接升级为写入锁的。因为获取一个写入锁需要释放所有读取锁，所以如果有两个读取锁视图获取写入锁而都不释放读取锁时就会发生死锁。

锁获取中断：
	读取锁和写入锁都支持获取锁期间被中断。这个和独占锁一致。

条件变量：
	写入锁提供了条件变量(Condition)的支持，这个和独占锁一致，但是读取锁却不允许获取条件变量，将得到一个UnsupportedOperationException异常。

重入数：
	读取锁和写入锁的数量最大分别只能是65535（包括重入数）。
	
	
读写锁的实现：

暂略。

(5)锁的其他问题：

所有的锁(内置锁和高级锁)都是有性能消耗的，在高并发的情况下，由于锁机制带来的上下文切换、资源同步等消耗是不可忽视的。在极端情况下，线程在锁上的消耗可能比线程本身的消耗还要多。所以，尽量少用锁，如果一定要使用锁，可以考虑非阻塞算法。

内置锁：

Java通过synchronized关键字来保证原子性。由于每个Object对象都有一个隐含的锁，也称为监视器对象，在进入synchronized之前自动获取此内部锁，而一旦离开此方式就会自动释放锁。这是一个独占锁，所有锁请求之间是互斥的。synchronized语法较为简单，且在1.6之后经过优化，性能有了巨大的提升，接近高级锁的性能。只有在需要使用到高级锁特殊功能时才使用高级锁。

性能：

强制的独占锁会急剧地降低Web吞吐量。

线程阻塞：

锁竞争的时候，失败的线程必然会发生阻塞。JVM既能自旋等待(不断尝试，直到成功)，也可以在操作系统中挂起阻塞的线程。自旋等待适合比较短的等待，而线程挂起比较适合那些比较耗时的等待。

锁竞争：

影响锁竞争的条件有两个：锁被请求的频率和每次持有锁的时间。当两者都很小的时候，锁竞争不会成为主要的性能瓶颈。但是如果锁使用不当，导致两者都比较大，那么很有可能CPU不能有效处理任务，导致任务被大量堆积。

减少锁竞争的方式有：

减少锁持有的时间
减少锁请求的频率
采用共享锁代替独占锁

死锁：

如果一个线程永远不释放另外一个线程需要的资源那么就会导致死锁。

有两种情况：

一种情况是线程A永远不释放锁，结果B一直拿不到锁，所以线程B就死掉了。
另一种是线程A拥有线程B需要的锁，同时线程B拥有线程A需要的锁，那么这时线程A/B互相依赖对方释放锁，于是两个线程都死掉了。

线程饥饿死锁：一个线程总是不能被调度，那么等待此线程结果的线程可能就死锁了。这种情况称之为饥饿死。

避免死锁的解决方案：

尽可能地按照锁的使用规范请求锁，另外锁的请求粒度要小(不要在不需要锁的地方占用锁，锁不用了尽快释放)。
在高级锁里面总是使用tryLock或者定时机制(指定获取锁超时时间)。

活锁：

活锁描述的是线程总是尝试某项操作却总是失败的情况，尽管线程没有被阻塞，但任务却总是不能被执行。

另外一种情况是在一个队列中每次从队列头取出一个任务来执行，每次都失败，然后将任务放入队列头，接下来再一次从队列头取出任务执行，仍然失败。

还有一种情况是发生在"碰撞协让"情况下：两个人过独木桥，如果在半路相撞，双方礼貌退出然后再试一次。如果总是失败，那么这两个任务将一直无法得到执行。


总之，解决锁问题的关键就是：从简单开始，先保证正确，然后再开始优化。

6.并发容器：

jdk1.4之前只有Vector和Hashtable是线程安全的集合(也称并发容器，Collections.synchronized*系列也可以看作是线程安全的实现)。从jdk5开始增加了线程安全的 Map 接口和线程安全的 BlockingQueue 。

(1)ConcurrentMap：

Map的体系结构：参考体系图。

Hashtable继承的是 Dictionary ，并不继承 AbstractMap 或者 HashMap。ConcurrentHashMap 是 HashMap 的线程安全版本，ConcurrentSkipListMap 是 TreeMap 的线程安全版本。Hashtable 已经过时，最终可用的安全容器应该是：ConcurrentHashMap 和  ConcurrentSkipListMap。

ConcurrentHashMap 既实现了 Map 接口中的方法，也实现了 ConcurrentMap 里面的方法：

V putIfAbsent(K key, V value)：如果不存在key对应的值，则将value以key加入Map，否则返回key对应的旧值。
boolean remove(Object key, Object value)：只有目前将键的条目映射到给定值时，才移除该键的条目。
boolean replace(K key, V oldValue, V newValue)：只有目前将键的条目映射到给定值时，才替换该键的条目。
V replace(K key, V value)：只有当前键存在的时候更新此键对于的值。

HashMap原理：略。

ConcurrentHashMap 和 ConcurrentSkipListMap 原理：略。

(2)Queue：队列。

Queue 是 jdk1.5之后引入的新的集合类，属于 Java Collections Framework，和 List/Set 同一级别，通常来讲 Queue 描述的是一种 FIFO 的队列。

对于 Queue 来说，在 Collection 的基础上增加了 offer/remove/poll/element/peek 方法，并重新定义了 add 方法：

      抛出异常  返回特殊值  操作描述
插入  add(e)    offer(e)    将元素加入到队列尾部
移除  remove()  poll()      移除队列头部的元素
检查  element() peek()      返回队列头部的元素而不移除此元素

对于 Queue 来说，并没有规定 Queue 队列是线程安全的，因此引入了 BlockingQueue ，BlockingQueue队列操作可以被阻塞，所有的操作都是线程安全的。

Deque 是 Queue 的另一个子接口，描述的是双向队列，Deque 允许在队列的头部增加元素和在队列的尾部删除元素，BlockingDeque 是线程安全的双向队列。

LinkedList 实现的本身 就是一种双向队列，所以jdk1.5之后就将 LinkedList 同时实现了 Deque 接口，所以 LinkedList 也属于 Queue 的一部分。

ArrayDeque：Queue通常是依靠链表结构来实现的，链表结构会带来额外的开销，双向链表的开销会更大，为了节省内存，使用固定大小的数组来实现队列。ArrayDeque 就是这样一种实现。
ArrayBlockingQueue：也是一种数组实现的队列，但却不是双向的。
PriorityQueue、PriorityBlockingQueue：实现了一种排序的队列模型，类似于 SortedSet，通过队列的 Comparator 或 Comparable 来排序元素，这种情况下元素在队列中的出入不是FIFO的，而是比较后的自然顺序来进行。
CocurrentLinkedQueue：一种线程安全却非阻塞的FIFO队列，是 Queue 的一个线程安全实现。按照 FIFO 原则对元素进行排序，队列头部时间最长，队列尾部时间最短，队列元素不允许为 null 。
SynchronousQueue：严格意义上并不是一种真正的队列。此队列维护一个线程列表，这些线程等待从队列中加入元素或者移除元素。能够最大线程的保持吞吐量却又是线程安全的。对于需要快速处理的任务队列是一个很好的选择。
DelayQueue：每个元素带有一个延时时间，当调用 take/poll 时会检测队列头元素这个时间是否<=0，若已经超时了，那么这个元素就可以被删除了。


BlockingQueue：

BlockingQueue 是 Queue 主要的线程安全版本。允许添加/删除元素被阻塞，直到成功为止。BlockingQueue 在 Queue 的基础上增加了 put/take 操作。

	    抛出异常    特殊值    阻塞    超时                  描述
插入    add(e)      offer(e)  put(e)  offer(e,time,unit)    将元素加入到队列尾部
移除    remove()    poll()    take()  poll(time,unit)       移除队列头部的元素
检查    element()   peek()                                  读取而不删除队列头部的元素

BlockingQueue 原理：略。


Deque：

Deque 允许在队列头部和尾部进行入队和出队操作，参考体系图。

Deque 在 Queue 的基础上增加了更多的方法，参考方法图。

Deque 不仅具有 FIFO 的 Queue实现，也有FILO的实现，也就是不仅可以实现队列，也可以实现堆栈。

LinkedList：

用 size 来记录大小，只有 head 一个 Entry，head 相当于双向链表的虚节点，虚节点的下一个元素是队列头元素，虚节点的上一个元素是队列的尾元素，虚节点在头和尾之间建立了一个通道，使整个队列形成了一个循环。

ArrayDeque 在遍历和节省内存上更高效，LinkedList 在扩容上更加灵活。


LinkedBlockingDeque：双向并发阻塞队列。参考数据结构图。

从数据结构和功能可以得出下面的结论：

①要想支持阻塞功能，队列的容量一定是固定的，否则无法在入队的时候挂起线程。也就是capacity是final类型的。
②既然是双向链表，每一个结点就需要前后两个引用，这样才能将所有元素串联起来，支持双向遍历。也即需要prev/next两个引用。
③双向链表需要头尾同时操作，所以需要first/last两个节点，当然可以参考LinkedList那样采用一个节点的双向来完成，那样实现起来就稍微麻烦点。
④既然要支持阻塞功能，就需要锁和条件变量来挂起线程。这里使用一个锁两个条件变量来完成此功能。

LinkedBlockingDeque的优缺点：

①功能足够强大，同时由于采用一个独占锁，因此实现起来也比较简单。所有对队列的操作都加锁就可以完成。同时独占锁也能够很好的支持双向阻塞的特性。
②性能上，LinkedBlockingDeque要比LinkedBlockingQueue要低很多，比CocurrentLinkedQueue就低更多了。


Exchanger：

jdk1.5之后开始的，两个工作线程之间交换数据的封装工具类，简单地说就是一个线程在完成一定的事务后想与另一个线程交换数据，则第一个先拿出数据的线程会一直等待第二个线程，直到第二个线程拿着数据到来时才能彼此交换对应数据。


对于 List 和 Set 而言，增删操作都是针对整个容器的，每次操作都阻塞整个容器空间，会极大地影响性能。读写锁是一种解决方案，CopyOnWriteArrayList/CopyOnWriteArraySet 是另一种解决方案(读操作不需要锁了)。

对于 CopyOnWriteArrayList/CopyOnWriteArraySet 有几点注意：

①ArrayList 仍然是基于数组的实现，因为只有数组是最快的。
②为了保证无锁的读操作能够看到写操作的变化，因此数组 array 是 volatile 类型的。
③get/indexOf/iterator 等操作都是无锁的，同时也可以看到操作的都是某一时刻 array 的镜像（这得益于数组是不可变化的）。
④add/set/remove/clear 等元素变化的都是需要加锁的，这里使用的是 ReentrantLock。

7.线程池：

线程池API：参考线程池API体系图。

大多数并发应用程序是围绕执行任务(Task)进行管理的。把一个应用程序的工作分离到任务中，可以简化程序的管理，这种分离还在不同事物间划分了自然的分界线，可以方便程序在出现错误时进行恢复，同时这种分离还可以为并行工作提供一个自然的结构，有利于提高程序的并发性。

任务：抽象、离散的工作单元。

任务拆分：确定每一个任务的执行边界。独立的工作单元有最大的吞吐量，这些工作单元不依赖其他工作单元的状态、结果或者其他资源。因此将任务尽可能地拆分成一个个独立的工作单元有利于提高程序的并发性。

对于有依赖关系以及资源竞争的工作单元就涉及到任务的调度和负载均衡。结果或者其他资源等有关联的工作单元就需要有一个总体的调度者来协调资源和执行顺序。同样在有限的资源情况下，大量的任务也需要一个协调各个工作单元的调度者。这就涉及到任务执行的策略问题。

任务的执行策略包括：4W3H。

①任务在什么(What)线程中执行。
②任务以什么(What)顺序执行(FIFO/LIFO/优先级等)。
③同时有多少个(How Many)任务并发执行。
④允许有多少个(How Many)个任务进入执行队列。
⑤系统过载时选择放弃哪一个(Which)任务，如何(How)通知应用程序这个动作。
⑥任务执行的开始、结束应该做什么(What)处理。

如何满足这些条件：

①在 Java 里面可以供使用者调用的启动线程类是 Thread 。因此 Runnable 或者 Timer/TimerTask 等都是要依赖 Thread 来启动的，因此在 ThreadPool 里面同样也是靠 Thread 来启动多线程的。
②Runnable 接口执行完毕后是不能拿到执行结果的，因此在 ThreadPool 里就定义了一个 Callable 接口来处理执行结果。
③为了异步阻塞的获取结果，Future 可以帮助调用线程获取执行结果。
④Executor 解决了向线程池提交任务的入口问题，同时 ScheduledExecutorService 解决了如何进行重复调用任务的问题。
⑤CompletionService 解决了如何按照执行完毕的顺序获取结果的问题，这在某些情况下可以提高任务执行的并发，调用线程不必在长时间任务上等待过多时间。
⑥线程的数量是有限的，而且也不宜过多，因此合适的任务队列是必不可少的，BlockingQueue 的容量正好可以解决此问题。
⑦固定任务容量就意味着在容量满了以后需要一定的策略来处理过多的任务(新任务)，RejectedExecutionHandler 正好解决此问题。
⑧一定时间内阻塞就意味着有超时，因此 TimeoutException 就是为了描述这种现象。TimeUnit 是为了描述超时时间方便的一个时间单元枚举类。
⑨有上述问题就意味了配置一个合适的线程池是很复杂的，因此 Executors 默认的一些线程池配置可以减少这个操作。


Java里面线程池的顶级接口是 Executor，严格意义上讲 Executor 并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是 ExecutorService。

线程池类体系结构：参考线程池类体系结构图。

Executor 的 execute 方法只是执行了一个 Runnable 任务。根据线程池的执行策略最后这个任务可能在新的线程中执行，或者线程池中的某个线程，甚至是在调用者线程中执行。

ExecutorService 在 Executor 的基础上增加了一些方法，包括两个最核心的方法：

Future<?> submit(Runnable task)
<T> Future<T> submit(Callable<T> task)

两个方法都是向线程池中提交任务，区别在于Runnable在执行完毕后没有结果，Callable在执行完毕后有一个结果。这在多个线程中传递状态和结果是非常有用的。另外他们的相同点在于都返回一个Future对象。Future对象可以阻塞线程直到运行完毕(获取结果，如果有的话)，也可以取消任务执行，当然也能够检测任务是否被取消或者是否执行完毕。在没有Future之前我们检测一个线程是否执行完毕通常使用Thread.join()或者用一个死循环加状态位来描述线程执行完毕。现在有了更好的方法能够阻塞线程，检测任务执行完毕甚至取消执行中或者未开始执行的任务。

ScheduledExecutorService 继承自 ExecutorService，描述的功能和 Timer/TimerTask 类似，解决那些需要任务重复执行的问题。这包括延迟时间一次性执行、延迟时间周期性执行以及固定延迟时间周期性执行等。
ThreadPoolExecutor：是 ExecutorService 的默认实现，其中的配置、策略也是比较复杂的，在后面分析。
ScheduledThreadPoolExecutor：是继承 ThreadPoolExecutor 的 ScheduledExecutorService 接口实现，周期性任务调度的类实现。
CompletionService：用于描述顺序获取执行结果的一个线程池包装器。依赖一个具体的线程池调度，但是能够根据任务的执行先后顺序得到执行结果，这在某些情况下可能提高并发效率。


在 Executors 类里面提供了一些静态工厂，生成一些常用的线程池：

newSingleThreadExecutor：创建一个单线程的线程池。这个线程池只有一个线程工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它，此线程池保证所有任务的执行顺序按照任务的提交顺序执行。
newFixedThreadPool：创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。
newCachedThreadPool：创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。
newScheduledThreadPool：创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。
newSingleThreadScheduledExecutor：创建一个单线程的线程池。此线程池支持定时以及周期性执行任务的需求。


Executor 的生命周期：

JVM 会在所有非后台守护线程终止之后才退出，线程池 Executor 是异步执行任务的，因此任何时刻都不能够直接地获取提交任务的状态。关闭线程池之后应该有相应的反馈信息。

关闭线程池可能会出现：

平缓关闭：已经启动的任务全部执行完毕，同时不再接受新的任务。
立即关闭：取消所有正在执行和未执行的任务。

线程池的四种状态：

①线程池在构造前(new操作)是初始状态，一旦构造完成线程池就进入了执行状态RUNNING。严格意义上讲线程池构造完成后并没有线程被立即启动，只有进行"预启动"或者接收到任务的时候才会启动线程。线程池是处于运行状态，随时准备接受任务来执行。
②线程池运行中可以通过shutdown()和shutdownNow()来改变运行状态。shutdown()是一个平缓的关闭过程，线程池停止接受新的任务，同时等待已经提交的任务执行完毕，包括那些进入队列还没有开始的任务，这时候线程池处于SHUTDOWN状态；shutdownNow()是一个立即关闭过程，线程池停止接受新的任务，同时线程池取消所有执行的任务和已经进入队列但是还没有执行的任务，这时候线程池处于STOP状态。
③一旦shutdown()或者shutdownNow()执行完毕，线程池就进入TERMINATED状态，此时线程池就结束了。
④isTerminating()描述的是SHUTDOWN和STOP两种状态。
⑤isShutdown()描述的是非RUNNING状态，也就是SHUTDOWN/STOP/TERMINATED三种状态。

线程池的API：参考 ExecutorService-API.png。

shutdownNow()会返回那些已经进入了队列但是还没有执行的任务列表。awaitTermination描述的是等待线程池关闭的时间，如果等待时间线程池还没有关闭将会抛出一个超时异常。对于关闭线程池期间发生的任务提交情况就会触发一个拒绝执行的操作。这是java.util.concurrent.RejectedExecutionHandler描述的任务操作。

总的来说：

①线程池有运行、关闭、停止、结束四种状态，结束后就会释放所有资源。
②平缓关闭线程池使用shutdown()。
③立即关闭线程池使用shutdownNow()，同时得到未执行的任务列表。
④检测线程池是否正处于关闭中，使用isShutdown()。
⑤检测线程池是否已经关闭，使用isTerminated()。
⑥定时或者永久等待线程池关闭结束，使用awaitTermination()操作。


RejectedExecutionHandler：提供了四种方式来处理任务拒绝策略。参考 RejectedExecutionHandler策略图。


周期性任务调度：

jdk1.5之前，java.util.Timer/TimerTask是唯一的内置任务调度方法，Timer/TimerTask的特性(或者说缺陷)如下：

①Timer对任务的调度是基于绝对时间的。
②所有的TimerTask只有一个线程TimerThread来执行，因此同一时刻只有一个TimerTask在执行。
③任何一个TimerTask的执行异常都会导致Timer终止所有任务。
④由于基于绝对时间并且是单线程执行，因此在多个任务调度时，长时间执行的任务被执行后有可能导致短时间任务快速在短时间内被执行多次或者干脆丢弃多个任务。

java.util.concurrent.ScheduledExecutorService 弥补了 Timer/TimerTask 的缺陷。

ScheduledExecutorService 基于 ExecutorService，是一个完整的线程池调度，另外在提供线程池的基础上增加了四个调度任务的API。

①schedule(Runnable command, long delay, TimeUnit unit)：在指定的延迟时间一次性启动任务(Runnable)，没有返回值。
②schedule(Callable<V> callable, long delay, TimeUnit unit)：在指定的延迟时间一次性启动任务(Callable)，携带一个结果。
③scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)：创建并执行一个在给定初始延迟后首次启用的定期操作，后续操作具有给定的周期，也就是将在 initialDelay 后开始执行，然后在 initialDelay+period 后执行，接着在 initialDelay + 2 * period 后执行，依此类推。如果任务的任何一个执行遇到异常，则后续执行都会被取消。否则，只能通过执行程序的取消或终止方法来终止该任务。如果此任务的任何一个执行要花费比其周期更长的时间，则将推迟后续执行，但不会同时执行。
④scheduleWithFixedDelay(Runnable command,long initialDelay,long delay,TimeUnit unit)：创建并执行一个在给定初始延迟后首次启用的定期操作，随后，在每一次执行终止和下一次执行开始之间都存在给定的延迟。如果任务的任一执行遇到异常，就会取消后续执行。否则，只能通过执行程序的取消或终止方法来终止该任务。

上述方法解决了这些问题：

①ScheduledExecutorService任务调度是基于相对时间，不管是一次性任务还是周期性任务都是相对于任务加入线程池(任务队列)的时间偏移。
②基于线程池的ScheduledExecutorService允许多个线程同时执行任务，这在添加多种不同调度类型的任务是非常有用的。
③同样基于线程池的ScheduledExecutorService在其中一个任务发生异常时会退出执行线程，但同时会有新的线程补充进来执行。
④ScheduledExecutorService可以做到不丢失任务。

ScheduledExecutorService 可以完全替代 Timer/TimerTask。


线程池的实现及原理：略。


并发操作异常体系：

如果线程在调用 Object 类的 wait()、wait(long) 或 wait(long, int) 方法，或者该类的 join()、join(long)、join(long, int)、sleep(long) 或 sleep(long, int) 方法过程中受阻，则其中断状态将被清除，它还将收到一个 InterruptedException。
检测一个线程的中断状态描述是这样的：

Thread.interrupted()：

测试当前线程是否已经中断。线程的中断状态由该方法清除。换句话说，如果连续两次调用该方法，则第二次调用将返回false(在第一次调用已清除了其中断状态之后，且第二次调用检验完中断状态前，当前线程再次中断的情况除外)。也就是说如果检测到一个线程已经被中断了，那么线程的使用方(挂起、等待或者正在执行)都将应该得到一个中断异常，同时将会清除异常中断状态。


8.并发总结：

(1)死锁与活跃度：

死锁：

使用锁来保证线程安全，也会引起一些问题：

①锁顺序死锁：多个线程试图通过不同的顺序获得多个相同的资源，则发生的循环锁依赖现象。
②动态的锁顺序死锁：多个线程通过传递不同的锁造成的锁顺序死锁问题。
③资源死锁：线程间相互等待对方持有的锁，并且谁都不会释放自己持有的锁发生的死锁。也就是说当现场持有和等待的目标成为资源，就有可能发生此死锁。这和锁顺序死锁不一样的地方是，竞争的资源之间并没有严格先后顺序，仅仅是相互依赖而已。
资源死锁还包括一种线程饥饿死锁，提交到线程池中的任务总是不能够抢到线程从而一直不被执行，造成任务"假死"状态。

避免和解决死锁：

①减小锁的范围。
②锁使用后尽快地释放锁。
③减少锁之间的依赖及遵守获取锁的顺序。
④尽可能地使用定时锁。


活跃度：

饥饿：需要访问的资源被永久拒绝，以至于不能再继续进行。

如：线程权重较低一直抢不到CPU资源。固定大小的线程池被使用完。

对于饥饿来说，需要平衡资源的竞争，空闲资源较多时，发生饥饿的可能性就越小。

弱响应性：线程最终能够有效地执行，只是等待的响应时间较长。

如：一个线程长时间独占一个锁，其他线程被迫等待。

活锁：指线程虽然没有被阻塞，但由于条件不足，一直尝试重试，却最终失败。

解决活锁，可以从修改重试机制的随机性和限制重试次数入手。

(2)常见的并发场景：

线程池：

可以有效地提高吞吐量，最常见的是Web容器的线程池。Web容器使用线程池处理HTTP请求。

任务队列：

消峰、限流。

异步处理：

略。

同步操作：

正确地使用原子操作，合理地使用独占锁和读写锁。

分布式锁：

非常难以处理，略。

(3)常见的并发陷阱：

volatile：只能保证数据的可见性，并不能保证原子操作和线程安全。

最常见于：

①循环检测机制：

	volatile boolean done = false;

    while(!done){
		dosomething();
    }

②单例模型：

public class DoubleLockSingleton {
    private static volatile DoubleLockSingleton instance = null;

    private DoubleLockSingleton() {
    }

    public static DoubleLockSingleton getInstance() {
        if (instance == null) {
            synchronized (DoubleLockSingleton.class) {
                if (instance == null) {
                    instance = new DoubleLockSingleton();
                }
            }
        }
        return instance;
    }
}

注意：

这里双重检测的原因：

外层检测，防止每次获取单例都进行同步操作。

内层检测：

instance = new DoubleLockSingleton(); 包括：分配内存空间、创建对象、instance引用指向对象。正常的顺序是1->2->3，由于线程调度，线程A先执行了1->3，让出时间片，其他线程可能看到已经存在但不完整的实例。

synchronized/Lock：

对于简单的逻辑使用 synchronized 完全没有问题，而且从代码结构上是更简单的。

对于比较复杂的逻辑，如：读写锁、条件变量、更高的吞吐量以及更加灵活、动态的用法，可以考虑使用Lock。

Lock的正确用法：

Lock lock = ...;
lock.lock();
try{
    //do something
}finally{
    lock.unlock();
}

如果担心发生死锁，无法恢复，可以使用尝试锁 tryLock() ：

Lock lock = ...;
if(lock.tryLock()){
    try{
        //do something
    }finally{
        lock.unlock();
    }
}

甚至可以使用限定超时的锁机制：

Lock.tryLock(long,TimeUnit)


锁的边界：

一个典型的错误是这样的：

ConcurrentMap<String,String> map = new ConcurrentHashMap<String,String>();
if(!map.containsKey(key)){
    map.put(key,value);
}

先检测再加入，两个原子操作合起来并不是原子操作。实际上可以使用：putIfAbsent(K, V) 的原子操作机制，等价于：

if(map.containsKey(key)){
    return map.get(key);
}else{
    return map.put(k,v);
}


构造函数启动线程：

禁止在构造函数中启动线程。


丢失通知问题：

对于wait/notify/notifyAll以及await/singal/singalAll，如果不确定到底是否能够正确的收到消息，担心丢失通知，简单一点就是总是通知所有。如果担心只收到一次消息，使用循环一直监听是不错的选择。


线程数：

并不是线程数越多越好，选择一个合适的线程数有助于提高吞吐量。

(4)性能与伸缩性：

使用线程的一种说法是为了提高性能。多线程可以使程序充分利用闲置的资源，提高资源的利用率，同时能够并行处理任务，提高系统的响应性。但是很显然，引入线程的同时也引入了系统的复杂性。另外系统的性能并不是总是随着线程数的增加而总是提高。

性能：性能的提升通常意味着可以用更少的资源做更多的事情。

可伸缩性：增加计算资源、吞吐量和生产量相应得到的改进。

Amdahl定律：并行的任务增加资源显然能够提高性能，但是如果是串行的任务，增加资源并不一定能够得到合理的性能提升。Amdahl定律描述的是在一个系统中，增加处理器资源对系统性能的提升比率。

性能提升：可以从以下这些方面入手。

①系统平台的资源利用率：指某一个设备繁忙且服务于此程序的时间占所有时间的比率。
②延迟：完成任务所耗费的时间。
③多处理：在单一系统上同时执行多个进程或者多个程序的能力。
④多线程：同一个地址空间内同时执行多个线程的过程。
⑤并发性：同时执行多个程序或者任务称之为并发。
⑥吞吐量：衡量系统在单位之间内可以完成的工作总量。
⑦瓶颈：程序运行过程中性能最差的地方。
⑧可扩展性：指程序或系统通过增加可使用的资源而增加性能的能力。

线程开销：

假设引入的多线程都用于计算，那么性能一定会有很大的提升么？其实引入多线程以后也会引入更多的开销。

切换上下文：

切换线程会导致上下文切换。线程的调度会导致CPU需要在操作系统和进程间花费更多的时间片段，这样真正执行应用程序的时间就减少了。另外上下文切换也会导致缓存的频繁进出，对于一个刚被切换的线程来说，可能由于高速缓冲中没有数据而变得更慢，从而导致更多的IO开销。

内存同步：

不同线程间要进行数据同步，synchronized 以及 volatile 提供的可见性都会导致缓存失效。线程栈之间的数据要和主存进行同步，这些同步有一些小小的开销。如果线程间同时要进行数据同步，那么这些同步的线程可能都会受阻。

阻塞：

当发生锁竞争时，失败的线程会导致阻塞。通常阻塞的线程可能在JVM内部进行自旋等待，或者被操作系统挂起。自旋等待可能会导致更多的CPU切片浪费，而操作系统挂起则会导致更多的上下文切换。


总的来说：保证逻辑正确的情况下，找到性能瓶颈，小步改进和优化。



























